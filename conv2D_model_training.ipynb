{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyhht"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU found\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "if tf.test.gpu_device_name():\n",
    "    print('GPU found')\n",
    "else:\n",
    "    print(\"No GPU found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib\n",
    "# Force matplotlib to not use any Xwindows backend.\n",
    "matplotlib.use('Agg')\n",
    "# import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.models import Sequential, model_from_json, load_model\n",
    "from keras.layers.core import Dense, Dropout, Flatten, Activation, SpatialDropout2D, Reshape, Lambda\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import ELU, PReLU, LeakyReLU\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "import pdb\n",
    "import os\n",
    "from os.path import join as ojoin\n",
    "import time  \n",
    "import numpy as np\n",
    "import numpy.matlib\n",
    "import argparse\n",
    "import random\n",
    "\n",
    "from pyhht.visualization import plot_imfs\n",
    "import IPython.display as ipd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\jupyter_notebook\\\\denoise'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filepaths(directory):\n",
    "    \"\"\"\n",
    "    This function will generate the file names in a directory \n",
    "    tree by walking the tree either top-down or bottom-up. For each \n",
    "    directory in the tree rooted at directory top (including top itself), \n",
    "    it yields a 3-tuple (dirpath, dirnames, filenames).\n",
    "    \"\"\"\n",
    "    file_paths = []  # List which will store all of the full filepaths.\n",
    "\n",
    "    # Walk the tree.\n",
    "    for root, directories, files in os.walk(directory):\n",
    "        for filename in files:\n",
    "            if filename.endswith('.wav'):\n",
    "            # Join the two strings in order to form the full filepath.\n",
    "                filepath = os.path.join(root, filename)\n",
    "                file_paths.append(filepath)  # Add it to the list.\n",
    "                # pdb.set_trace()\n",
    "    return file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5760,\n",
       " 5760,\n",
       " ['mixed/mixed_all_snr/hsiaoen_T10-10_20200622_toll_bar_snr6.wav',\n",
       "  'mixed/mixed_all_snr/hsiaoen_T10-1_20200622_fall_streets1_snr-10.wav',\n",
       "  'mixed/mixed_all_snr/hsiaoen_T10-2_20200622_summer_beach2_snr-6.wav',\n",
       "  'mixed/mixed_all_snr/hsiaoen_T10-3_20200622_brook_in_a_cave_snr-11.wav',\n",
       "  'mixed/mixed_all_snr/hsiaoen_T10-4_20200622_washing_machine2_snr-5.wav',\n",
       "  'mixed/mixed_all_snr/hsiaoen_T10-5_20200622_strong_wind1_snr-5.wav',\n",
       "  'mixed/mixed_all_snr/hsiaoen_T10-6_20200622_brook_in_a_cave_snr-11.wav',\n",
       "  'mixed/mixed_all_snr/hsiaoen_T10-7_20200622_ountain_children_snr4.wav',\n",
       "  'mixed/mixed_all_snr/hsiaoen_T10-8_20200622_ort1_snr-3.wav',\n",
       "  'mixed/mixed_all_snr/hsiaoen_T10-9_20200622_summer_beach1_snr5.wav'],\n",
       " ['processed_data/hsiaoen\\\\hsiaoen_T10-10_20200622.wav',\n",
       "  'processed_data/hsiaoen\\\\hsiaoen_T10-1_20200622.wav',\n",
       "  'processed_data/hsiaoen\\\\hsiaoen_T10-2_20200622.wav',\n",
       "  'processed_data/hsiaoen\\\\hsiaoen_T10-3_20200622.wav',\n",
       "  'processed_data/hsiaoen\\\\hsiaoen_T10-4_20200622.wav',\n",
       "  'processed_data/hsiaoen\\\\hsiaoen_T10-5_20200622.wav',\n",
       "  'processed_data/hsiaoen\\\\hsiaoen_T10-6_20200622.wav',\n",
       "  'processed_data/hsiaoen\\\\hsiaoen_T10-7_20200622.wav',\n",
       "  'processed_data/hsiaoen\\\\hsiaoen_T10-8_20200622.wav',\n",
       "  'processed_data/hsiaoen\\\\hsiaoen_T10-9_20200622.wav'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fix the order of datas and labels\n",
    "mixed_file = sorted(get_filepaths('mixed/mixed_all_snr/'))\n",
    "cleaned_file = sorted(get_filepaths('processed_data/'))\n",
    "len(mixed_file), len(cleaned_file), mixed_file[2010:2020], cleaned_file[2010:2020]    # check datas corresponding to labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let datas separated into training and testing\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(mixed_file, cleaned_file, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4032, 4032, 1728, 1728)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train_Noisy_lists=X_train\n",
    "Train_Clean_paths= y_train\n",
    "\n",
    "Val_Noisy_lists  = X_val\n",
    "Val_Clean_paths = y_val\n",
    "          \n",
    "Num_valdata=len(Val_Noisy_lists)   \n",
    "Num_traindata=len(Train_Noisy_lists)\n",
    "\n",
    "# check the length of training datas and validation\n",
    "len(Train_Noisy_lists), len(Train_Clean_paths), len(Val_Noisy_lists), len(Val_Clean_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88800"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the max length of voice data\n",
    "max_len = 0\n",
    "for data in Train_Noisy_lists:\n",
    "    noisy, rate = librosa.load(data, sr=16000, mono=True)\n",
    "    if noisy.shape[0] > max_len:\n",
    "        max_len = noisy.shape[0]\n",
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4032"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noisy_stft_list = []\n",
    "for data in Train_Noisy_lists:\n",
    "    noisy, rate = librosa.load(data, sr=16000, mono=True, dtype='float32')\n",
    "    noisy = np.hstack((noisy, np.zeros(max_len - noisy.shape[0])))\n",
    "    noisy = librosa.stft(noisy, n_fft=512, hop_length=256, win_length=512, center=False)\n",
    "    noisy0 = np.abs(noisy)\n",
    "    noisy_stft_list += [noisy0]\n",
    "\n",
    "clean_stft_list = []\n",
    "for data in Train_Clean_paths:\n",
    "    noisy, rate = librosa.load(data, sr=16000, mono=True, dtype='float32')\n",
    "    noisy = np.hstack((noisy, np.zeros(max_len - noisy.shape[0])))\n",
    "    noisy = librosa.stft(noisy, n_fft=512, hop_length=256, win_length=512, center=False)\n",
    "    noisy0 = np.abs(noisy)\n",
    "    noisy_stft_list += [noisy0]\n",
    "len(noisy_stft_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_data_generator(noisy_list, clean_path):\n",
    "    index=0\n",
    "    while True:\n",
    "        noisy, rate = librosa.load(data, sr=16000, mono=True, dtype='float32')\n",
    "        noisy = np.hstack((noisy, np.zeros(max_len - noisy.shape[0])))\n",
    "        noisy = librosa.stft(noisy, n_fft=512, hop_length=256, win_length=512, center=False)\n",
    "        noisy0 = np.abs(noisy)\n",
    "        noisy_stft_list += [noisy0]\n",
    "        noisy = noisy.astype('float32')\n",
    "        noisy = scipy.signal.stft(noisy, nperseg=512, nfft=512, fs=16000, noverlap=256)\n",
    "        noisy0 = np.abs(noisy)\n",
    "        noisy0 = np.reshape(noisy0,(1, np.shape(noisy0)[0], np.shape(noisy0)[1],1))\n",
    "        \n",
    "        rate, clean = wavfile.read(clean_path[index])\n",
    "        clean = clean.astype('float32')\n",
    "        sr, _, clean = scipy.signal.stft(clean, nperseg=512, nfft=512, fs=16000, noverlap=256)\n",
    "        clean0 = np.abs(clean)\n",
    "        clean0 = np.reshape(clean0,(1, np.shape(clean0)[0], np.shape(clean0)[1],1))\n",
    "\n",
    "        index += 1\n",
    "        if index == len(noisy_list):\n",
    "            index = 0\n",
    "        yield noisy0, clean0\n",
    "\n",
    "def val_data_generator(noisy_list, clean_path):\n",
    "    index=0\n",
    "    while True:\n",
    "         #noisy, rate  = librosa.load(noisy_list[index],sr=16000)       \n",
    "        rate, noisy = wavfile.read(noisy_list[index])\n",
    "        noisy = noisy.astype('float32')\n",
    "        sr, _, noisy = scipy.signal.stft(noisy, nperseg=512, nfft=512, fs=16000, noverlap=256)\n",
    "        noisy0 = np.abs(noisy)\n",
    "        noisy0 = np.reshape(noisy0,(1, np.shape(noisy0)[0], np.shape(noisy0)[1],1))\n",
    "          \n",
    "        rate, clean = wavfile.read(clean_path[index])\n",
    "        clean = clean.astype('float32')\n",
    "        sr, _, clean = scipy.signal.stft(clean, nperseg=512, nfft=512, fs=16000, noverlap=256)\n",
    "        clean0 = np.abs(clean)\n",
    "        clean0 = np.reshape(clean0,(1, np.shape(clean0)[0], np.shape(clean0)[1],1))\n",
    "\n",
    "        index += 1\n",
    "        if index == len(noisy_list):\n",
    "            index = 0\n",
    "        yield noisy0, clean0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters = 32, kernel_size = (5,5), padding = 'Same', activation ='relu', input_shape = (257, None, 1), kernel_initializer='he_normal'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(filters = 64, kernel_size = (3,3), padding = 'Same', activation ='relu', kernel_initializer='he_normal'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(filters = 128, kernel_size = (3,3), padding = 'Same', activation ='relu', kernel_initializer='he_normal'))\n",
    "model.add(BatchNormalization())\n",
    "# model.add(Conv2D(filters = 256, kernel_size = (3,3), padding = 'Same', activation ='relu', kernel_initializer='he_normal'))\n",
    "# model.add(Conv2D(filters = 512, kernel_size = (3,3), padding = 'Same', activation ='relu', kernel_initializer='he_normal'))\n",
    "model.add(Conv2D(filters = 256, kernel_size = (3,3), padding = 'Same', activation ='relu', kernel_initializer='he_normal'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(filters = 128, kernel_size = (3,3), padding = 'Same', activation ='relu', kernel_initializer='he_normal'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(filters = 64, kernel_size = (3,3), padding = 'Same', activation ='relu', kernel_initializer='he_normal'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', activation ='relu', kernel_initializer='he_normal'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(filters = 1, kernel_size = (5,5),padding = 'Same', activation ='tanh', kernel_initializer='he_normal'))\n",
    "\n",
    "model.summary()\n",
    "print('finished!!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 5\n",
    "batch_size = 1\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "with open('{}.json'.format('firsttry'),'w') as f:    # save the model\n",
    "    f.write(model.to_json()) \n",
    "checkpointer = ModelCheckpoint(filepath='./model_h5/{}.hdf5'.format('stft3_0714'), verbose=1, save_best_only=True, mode='min')  \n",
    "\n",
    "print ('training...')\n",
    "\n",
    "g1 = train_data_generator(Train_Noisy_lists, Train_Clean_paths)\n",
    "g2 = val_data_generator(Test_Noisy_lists, Test_Clean_paths)\n",
    "\n",
    "tbCallBack = TensorBoard(log_dir='./logs',  # log 目錄\n",
    "                 histogram_freq=0,  # 按照何等频率（epoch）來計算直方圖，0為不計算\n",
    "                 batch_size=32,     # 用多大量的數據計算直方圖\n",
    "                 write_graph=True,  # 是否存储網路結構圖\n",
    "                 write_grads=True, # 是否可視化梯度直方圖\n",
    "                 write_images=True,# 是否可視化參數\n",
    "                 embeddings_freq=0,\n",
    "                 embeddings_layer_names=None,\n",
    "                 embeddings_metadata=None)\n",
    "hist=model.fit_generator(g1,\n",
    "                         samples_per_epoch=Num_traindata,\n",
    "                        # samples_per_epoch=50,\n",
    "                        nb_epoch=epoch,\n",
    "                        verbose=1,\n",
    "                         validation_data=g2,\n",
    "                         nb_val_samples=Num_testdata,\n",
    "                         max_q_size=1,\n",
    "                         nb_worker=1,\n",
    "                         pickle_safe=False,\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the learning curve\n",
    "TrainERR=hist.history['loss']\n",
    "ValidERR=hist.history['val_loss']\n",
    "print ('@%f, Minimun error:%f, at iteration: %i' % (hist.history['val_loss'][epoch-1], np.min(np.asarray(ValidERR)),np.argmin(np.asarray(ValidERR))+1))\n",
    "# print 'drawing the training process...'\n",
    "plt.clf()\n",
    "plt.figure(4)\n",
    "plt.plot(range(1,epoch+1),TrainERR,'b',label='TrainERR')\n",
    "plt.plot(range(1,epoch+1),ValidERR,'r',label='ValidERR')\n",
    "plt.xlim([1,epoch])\n",
    "plt.legend()\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('error')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "plt.savefig('Learning_curve_{}7.png'.format('FCN_firsttry'), dpi=150)\n",
    "\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the top ten datas\n",
    "for path in Test_Noisy_lists[:10]:\n",
    "    S = path.split('/')\n",
    "    wave_name = S[2]\n",
    "    rate, noisy = wavfile.read(path)\n",
    "    noisy = noisy.astype('float32')\n",
    "    sr, _, noisy = scipy.signal.stft(noisy, nperseg=512, nfft=512, fs=16000, noverlap=256)\n",
    "    noisy0 = np.abs(noisy)\n",
    "    n_phase = np.angle(noisy)\n",
    "\n",
    "    noisy0 = np.reshape(noisy0, (1, np.shape(noisy0)[0], np.shape(noisy0)[1], 1))\n",
    "    enhanced = np.squeeze(model.predict(noisy0, verbose=0, batch_size=batch_size))\n",
    "    Rec = np.multiply(enhanced, np.exp(1j * n_phase))\n",
    "    _, enhanced = scipy.signal.istft(Rec, nperseg=512,nfft=512,fs=16000,noverlap=256)\n",
    "    enhanced = enhanced * maxv\n",
    "\n",
    "    librosa.output.write_wav(os.path.join(\"predict_result/test7_0713/\", wave_name), (enhanced).astype(np.int16), 16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
